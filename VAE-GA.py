# ==============================================================================
# Virtuous-Cycle VAE-GA Optimization for Artificial Spin Ice Ground States
# ==============================================================================
#
# DESCRIPTION:
# This script implements a "virtuous cycle" optimization pipeline that combines a
# Variational Autoencoder (VAE) with a Genetic Algorithm (GA) to find the
# ground state configurations of artificial spin ice systems.
#
# WORKFLOW:
# 1.  Load Initial Dataset: An initial dataset of low-energy spin configurations,
#     generated by Monte Carlo simulated annealing, is loaded.
# 2.  Train VAE: A VAE is trained on this dataset to learn a compact,
#     low-dimensional latent space representation of the spin states.
# 3.  Optimize with GA: A GA explores this latent space to discover new
#     configurations with even lower energies.
# 4.  Augment and Retrain: The best configurations found by the GA are added
#     to the training dataset. The VAE is then retrained on this augmented,
#     higher-quality dataset.
# 5.  Iterate: This cycle of training and optimization is repeated, allowing the
#     pipeline to progressively focus on the true low-energy manifold and
#     discover the ground state.
#
# LIBRARIES:
# - TensorFlow, NumPy, Matplotlib, Pandas, re
#
# EXTERNAL DEPENDENCIES:
# - EVAE_Modules.py: Contains VAE-related helper functions (e.g., get_hmloss).
# - Other_Modules.py: Contains other helper functions.
# - A dataset directory containing .npy files (TrainData, Edges, etc.).
#
# ==============================================================================

import re
import matplotlib.ticker as mticker
import pandas as pd

# CustomLib imports
from EVAE_Modules import *
from Other_Modules import *


# --- Helper Function: File Save Management ---
def get_run_suffix(output_directory, base_filename_prefix, ext=".png"):
    """
    Calculates a numeric suffix for output files to prevent overwriting.
    Scans the output directory for files with the given prefix and finds the
    next available integer suffix (e.g., "_1", "_2").
    """
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)
        return "_1"
    regex = re.compile(f"_(\\d+){re.escape(ext)}$")
    max_suffix_num = 0
    base_file_exists = any(
        f.startswith(base_filename_prefix) and not regex.search(f)
        for f in os.listdir(output_directory)
    )
    for filename in os.listdir(output_directory):
        if filename.startswith(base_filename_prefix):
            match = regex.search(filename)
            if match:
                suffix_num = int(match.group(1))
                if suffix_num > max_suffix_num:
                    max_suffix_num = suffix_num
    if max_suffix_num == 0 and not base_file_exists:
        return "_1"
    return f"_{max_suffix_num + 1}"


def apply_suffix(save_path, suffix):
    """Applies the calculated suffix to a file path before the extension."""
    base, ext = os.path.splitext(save_path)
    return f"{base}{suffix}{ext}"


# --- Enhanced Visualization Helper Functions ---
def plot_energy_distributions_evolution(energy_data_dict, reference_energies, save_path, title_prefix="",
                                        deep_search_period=None):
    num_plots = len(energy_data_dict)
    if num_plots <= 1: return
    fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(8, 2.5 * num_plots), sharex=True)
    if num_plots == 1: axes = [axes]
    all_energies = np.concatenate(list(energy_data_dict.values()) + [reference_energies])
    min_bin_range = np.min(all_energies)
    ref_mean, ref_std = np.mean(reference_energies), np.std(reference_energies)
    max_bin_range = ref_mean + 3 * ref_std
    bins = np.linspace(min_bin_range, max_bin_range, 60)
    sorted_cycles = sorted(energy_data_dict.keys())
    for i, cycle in enumerate(sorted_cycles):
        ax = axes[i];
        energies = energy_data_dict[cycle]
        ax.hist(reference_energies, bins=bins, color='gray', alpha=0.6, density=True)
        ax.hist(energies, bins=bins, color='red', alpha=0.7, density=True, edgecolor='white', linewidth=0.5)
        ax.set_ylabel('Density');
        ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=3, prune='both'))
    axes[-1].set_xlabel(r'Energy ($\epsilon$)')
    plotted_cycles_str = ", ".join(map(str, sorted_cycles))
    title_str = f'Energy Distribution Evolution: {title_prefix}\n(Cycles: {plotted_cycles_str})'
    if deep_search_period:
        title_str += f' - Deep Search @{deep_search_period} cycles'
    fig.suptitle(title_str, fontsize=14)
    plt.tight_layout(rect=[0, 0, 1, 0.93])
    plt.savefig(save_path, dpi=300);
    print(f"Saved energy distribution evolution plot to: {save_path}");
    plt.close(fig)


def plot_minimum_energy_evolution(cycles, min_energies, save_path, title_prefix="", deep_search_period=None):
    plt.figure(figsize=(12, 7))
    plt.plot(cycles, min_energies, marker='o', linestyle='-', markersize=4, color='black')
    title_str = f'Minimum Energy Evolution: {title_prefix}'
    if deep_search_period:
        title_str += f'\n(Deep Search every {deep_search_period} cycles)'
    plt.title(title_str, fontsize=14);
    plt.xlabel('Cycle');
    plt.ylabel('Minimum Energy')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300);
    print(f"Saved minimum energy evolution plot to: {save_path}");
    plt.close()


def plot_energy_distribution(energy_values, cycle, save_dir, run_suffix):
    plt.figure(figsize=(10, 6))
    p1, p99 = np.percentile(energy_values, [1, 99]);
    padding = (p99 - p1) * 0.1
    x_min_lim, x_max_lim = (p1 - padding, p99 + padding)
    plt.hist(energy_values, bins=100, color='red', alpha=0.7, edgecolor='white', linewidth=0.5,
             range=(x_min_lim, x_max_lim))
    min_energy, max_energy = min(energy_values), max(energy_values)
    ticks = np.linspace(min_energy, max_energy, 8);
    tick_labels = [f"{tick:.6f}" for tick in ticks]
    plt.xticks(ticks, tick_labels, rotation=45, ha="right");
    plt.title(f"Energy Distribution after Cycle {cycle}")
    plt.xlabel("Energy");
    plt.ylabel("Frequency");
    plt.tight_layout()
    save_path = os.path.join(save_dir, f"energy_distribution_cycle_{cycle}.png")
    final_save_path = apply_suffix(save_path, run_suffix)
    plt.savefig(final_save_path);
    print(f"Saved cycle energy plot to: {final_save_path}");
    plt.close()


def save_spin_edge_csv(spin3d_list, edges, coes, output_csv: str, run_suffix: str):
    E = edges.shape[0]
    for i, sp in enumerate(spin3d_list):
        if sp.shape != (E, 3): raise ValueError(f"spin3d_list[{i}] shape error")
    spin_dict = {}
    for idx, sp3d in enumerate(spin3d_list):
        spin_dict[f"Sx_{idx}"] = sp3d[:, 0];
        spin_dict[f"Sy_{idx}"] = sp3d[:, 1];
        spin_dict[f"Sz_{idx}"] = sp3d[:, 2]
    spin_dict["X1"], spin_dict["Y1"], spin_dict["X2"], spin_dict["Y2"] = edges[:, 0], edges[:, 1], edges[:, 2], edges[:, 3]
    spin_dict["XC"], spin_dict["YC"] = coes[:, 0], coes[:, 1]
    df = pd.DataFrame(spin_dict);
    df.insert(0, "Index", np.arange(len(df)))
    final_output_csv = apply_suffix(output_csv, run_suffix)
    df.to_csv(final_output_csv, index=False, float_format='%.6f');
    print(f"Saved spin+edge CSV to: {final_output_csv}")


# --- Main Logic ---
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
gpus = tf.config.experimental.list_physical_devices("GPU")
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except RuntimeError as e:
        print(e)

# --- Hyperparameters and Configuration ---
batch_size = 500;
encoder_hidden_units = [512, 384, 256];
decoder_hidden_units = [256, 384, 512]
encoding_dim = 128;
activation = "leakyrelu";
rcloss_type = "mse";
batchNorm_bl = True;
learning_rate = 1e-3;
gamma_fixed = 0

dataset_dir_name = "Sim_Cols10Rows10_DR1000.00"
# NOTE: The 'D' directory is hardcoded here. Adjust if necessary.
dataset_dir_path = os.path.join(os.getcwd(), "D", dataset_dir_name)
dip = 1.0;
dipRange = float(dataset_dir_name.split('DR')[-1])
print(f"Dipole constant: {dip}, Dipole range: {dipRange}")

# --- Data Loading ---
x_train_initial_3d = np.load(os.path.join(dataset_dir_path, "TrainData.npy"))
edges = np.load(os.path.join(dataset_dir_path, "Edges.npy"));
coes = np.load(os.path.join(dataset_dir_path, "CenterOfEdges.npy"))
rotmat = np.load(os.path.join(dataset_dir_path, "Rotmat.npy"));
nnidx = np.load(os.path.join(dataset_dir_path, "NNIndices.npy")) if dipRange == 0.0 else None
nbofspins = x_train_initial_3d.shape[1]
print(f"Dataset imported. Number of spins = {nbofspins}")
os.makedirs(dataset_dir_name, exist_ok=True)

run_suffix = get_run_suffix(dataset_dir_name, "energy_distribution_cycle")
print(f"Using file suffix for this run: '{run_suffix}'")

x_train = spin2binary(x_train_initial_3d, rotmat)
x_train1 = x_train.reshape(-1, nbofspins)
print("Binarized x_train prepared. Shape:", x_train1.shape)

# --- VAE Model Definition ---
encoder_inputs = tf.keras.Input(shape=(nbofspins,));
x = encoder_inputs
for i, u in enumerate(encoder_hidden_units):
    x = tf.keras.layers.Dense(u)(x)
    if batchNorm_bl:
        x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU()(x)
z_mean = tf.keras.layers.Dense(encoding_dim, name="z_mean")(x);
z_log_var = tf.keras.layers.Dense(encoding_dim, name="z_log_var")(x)


class Sampling(tf.keras.layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs;
        return z_mean + tf.exp(0.5 * z_log_var) * tf.random.normal(tf.shape(z_mean))


z = Sampling()([z_mean, z_log_var]);
encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")
latent_inputs = tf.keras.Input(shape=(encoding_dim,));
x = latent_inputs
for i, u in enumerate(decoder_hidden_units):
    x = tf.keras.layers.Dense(u)(x)
    if batchNorm_bl:
        x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU()(x)
decoder_outputs = tf.keras.layers.Dense(nbofspins, activation='tanh')(x);
decoder = tf.keras.Model(latent_inputs, decoder_outputs, name="decoder")


class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, beta=1.0):
        super(VAE, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.beta = beta
        self.total_loss_tracker = tf.keras.metrics.Mean(name="loss")
        self.recon_loss_tracker = tf.keras.metrics.Mean(name="recon_loss")
        self.kl_loss_tracker = tf.keras.metrics.Mean(name="kl_loss")
        self.energy_loss_tracker = tf.keras.metrics.Mean(name="energy_loss")
        self.dip_energy_tracker = tf.keras.metrics.Mean(name="dip_energy")

    @property
    def metrics(self):
        return [self.total_loss_tracker, self.recon_loss_tracker, self.kl_loss_tracker, self.energy_loss_tracker,
                self.dip_energy_tracker]

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            if rcloss_type == "mse":
                recon_loss = get_mse_rcloss(data, reconstruction)
            else:
                recon_loss = get_bc_rcloss(data, reconstruction)

            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))

            dipEnergy, hmloss = get_hmloss(reconstruction, rotmat, dip, dipBasis)
            energy_loss = hmloss

            total_loss = tf.reduce_mean(recon_loss) + self.beta * kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        self.total_loss_tracker.update_state(total_loss)
        self.recon_loss_tracker.update_state(tf.reduce_mean(recon_loss))
        self.kl_loss_tracker.update_state(kl_loss)
        self.energy_loss_tracker.update_state(energy_loss)
        self.dip_energy_tracker.update_state(dipEnergy)

        return {m.name: m.result() for m in self.metrics}


# --- VAE Initialization and Compilation ---
vae = VAE(encoder, decoder, beta=0.005)
vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)

dipBasis = get_dipbasis(coes, dipRange, nnidx)
initial_energy, _ = get_hmloss(x_train, rotmat, dip, dipBasis);
initial_energy_np = initial_energy.numpy()


def fitness_func(latentcode):
    return -get_hmloss(vae.decoder(latentcode), rotmat, dip, dipBasis)[0]


# --- Main Optimization Loop ---
num_cycles = 2
deep_search_period = 2
evolution_plot_period = 1
min_energy_plot_period = 20
num_lowest_energies = 50
num_csv_save = 1

min_energy_tracker = [np.min(initial_energy_np)]
evolution_energy_data = {0: initial_energy_np}

for cycle in range(num_cycles):
    is_deep_search_cycle = (cycle + 1) % deep_search_period == 0
    if is_deep_search_cycle:
        total_iteration, initial_mutation_rate, final_mutation_rate = 1000, 1.0, 0.01
        print(f"\n>>> Starting Deep Search Cycle {cycle + 1} ({total_iteration} iterations) <<<")
    else:
        total_iteration, initial_mutation_rate, final_mutation_rate = 1000, 1.0, 1.0
        print(f"\n--- Starting Standard Cycle {cycle + 1}/{num_cycles} ---")

    vae.fit(x_train1, epochs=100, batch_size=batch_size, callbacks=[early_stopping], verbose=2)
    print(f"Cycle {cycle + 1}: VAE training complete.")

    ga = GeneticAlgorithm3(fitness_func, save_dir=os.path.join(dataset_dir_name, f"ga_cycle_{cycle}"), dim=128,
                           num_samples=500, num_elite=0, probability_method="linear",
                           selection_method="stochastic_remainder_selection", crossover_method="rank_based_adaptive",
                           mutation_method="rank_based_adaptive", k1=0.5)
    optimized_latent_codes = ga.run(generator=vae.decoder, total_iteration=total_iteration, sub_iteration=100,
                                    dataset_dir_name=dataset_dir_name, rotmat=rotmat, dip=dip, dipBasis=dipBasis,
                                    edges=edges, coes=coes, get_hmtotalloss=get_hmloss,
                                    save_spin_edge_csv=lambda *args: save_spin_edge_csv(*args, run_suffix=run_suffix),
                                    initial_mutation_rate=initial_mutation_rate,
                                    final_mutation_rate=final_mutation_rate)

    x_train_ga_output = vae.decoder(optimized_latent_codes)
    x_train_binary = tf.where(x_train_ga_output > 0, 1.0, -1.0)
    energy, _ = get_hmloss(x_train_binary, rotmat, dip, dipBasis);
    energy_np = energy.numpy()

    energy_dist_save_path = os.path.join(dataset_dir_name, f"energy_distribution_values_cycle_{cycle + 1}.npy")
    final_energy_dist_path = apply_suffix(energy_dist_save_path, run_suffix)
    np.save(final_energy_dist_path, energy_np)
    print(f"Saved GA-generated energy distribution to: {final_energy_dist_path}")

    min_energy_this_cycle = np.min(energy_np);
    min_energy_tracker.append(min_energy_this_cycle)

    print(f"Cycle {cycle + 1} GA Results - Mean E: {np.mean(energy_np):.6f}, Min E: {min_energy_this_cycle:.6f}")
    plot_energy_distribution(energy_np, cycle + 1, dataset_dir_name, run_suffix)


    def find_lowest_info(arr, n):
        n_eff = min(n, len(arr));
        if n_eff == 0: return np.array([]), np.array([], dtype=int)
        indices = np.argpartition(arr, n_eff - 1)[:n_eff];
        return arr[indices], indices


    lowest_energies, lowest_indices = find_lowest_info(energy_np, num_lowest_energies)
    print(f"Lowest {len(lowest_energies)} energies: {lowest_energies}")
    print(f"Indices of lowest {len(lowest_indices)} energies: {lowest_indices}")
    energy_counts = {};
    for e in lowest_energies:
        if e not in energy_counts: energy_counts[e] = 0
        energy_counts[e] += 1
    print("Counts per unique low energy:", energy_counts)

    is_evolution_plot_cycle = (cycle + 1) % evolution_plot_period == 0
    if is_evolution_plot_cycle or (cycle + 1) == num_cycles:
        evolution_energy_data[cycle + 1] = energy_np
        dist_plot_path = os.path.join(dataset_dir_name, f"energy_distribution_evolution_at_cycle{cycle + 1}.png")
        plot_energy_distributions_evolution(energy_data_dict=evolution_energy_data,
                                            reference_energies=initial_energy_np,
                                            save_path=apply_suffix(dist_plot_path, run_suffix),
                                            title_prefix=dataset_dir_name, deep_search_period=deep_search_period)

    is_min_energy_plot_cycle = (cycle + 1) % min_energy_plot_period == 0
    if is_min_energy_plot_cycle or (cycle + 1) == num_cycles:
        min_e_plot_path = os.path.join(dataset_dir_name, f"minimum_energy_evolution_at_cycle{cycle + 1}.png")
        plot_minimum_energy_evolution(cycles=list(range(cycle + 2)), min_energies=min_energy_tracker,
                                      save_path=apply_suffix(min_e_plot_path, run_suffix),
                                      title_prefix=dataset_dir_name, deep_search_period=deep_search_period)
        min_e_array_path = os.path.join(dataset_dir_name, f"minimum_energies_at_cycle{cycle + 1}.npy")
        final_min_e_array_path_suffixed = apply_suffix(min_e_array_path, run_suffix)
        np.save(final_min_e_array_path_suffixed, np.array(min_energy_tracker))
        print(f"Saved intermediate minimum energy array to: {final_min_e_array_path_suffixed}")

    x_train_selected_from_ga = tf.gather(x_train_binary, lowest_indices, axis=0)

    if num_csv_save > 0 and x_train_selected_from_ga.shape[0] > 0:
        best_spin_binary = x_train_selected_from_ga[0]
        best_spin_3d = binary2spin(best_spin_binary[tf.newaxis, ...], rotmat)[0]
        traincsv_dir_name = "train_csv";
        os.makedirs(traincsv_dir_name, exist_ok=True)
        output_csv_path = os.path.join(traincsv_dir_name, f"train_{dataset_dir_name}_cycle{cycle + 1}.csv")
        save_spin_edge_csv([best_spin_3d, best_spin_3d, best_spin_3d], edges, coes, output_csv_path, run_suffix)

    num_to_add = x_train_selected_from_ga.shape[0]
    if num_to_add > 0:
        x_train1 = np.concatenate((x_train1, x_train_selected_from_ga.numpy()), axis=0);
        np.random.shuffle(x_train1)

    # --- Training Data Snapshot Feature ---
    snapshot_dir = os.path.join(dataset_dir_name, "training_data_snapshots")
    os.makedirs(snapshot_dir, exist_ok=True)
    snapshot_filename = f"training_data_cycle_{cycle + 1}.npy"
    snapshot_save_path = os.path.join(snapshot_dir, snapshot_filename)
    np.save(apply_suffix(snapshot_save_path, run_suffix), x_train1)
    print(f"Saved training data snapshot to: {apply_suffix(snapshot_save_path, run_suffix)}")

print("\nAll cycles complete.")
